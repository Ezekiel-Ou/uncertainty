import json
import re
from typing import Union
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
from zhipuai import ZhipuAI
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import torch.nn.functional as F
import os
import matplotlib.pyplot as plt
import numpy as np

# --------------------- ç­”æ¡ˆè§„èŒƒåŒ–æ¨¡å— ---------------------
def normalize_answer(text: Union[str, list]) -> str:
    """
    ç­”æ¡ˆè§„èŒƒåŒ–å¤„ç†ï¼šæå–å…³é”®å®ä½“ã€æ•°å­—ï¼Œç»Ÿä¸€å°å†™æ ¼å¼
    """
    if isinstance(text, list):
        text = " ".join(text)

    # æå–æ‰€æœ‰æ•°å­—å’Œå­—æ¯ç»„æˆçš„å®ä½“ï¼ˆè¿‡æ»¤æ ‡ç‚¹ï¼‰
    numbers = re.findall(r"\d+", text)
    entities = re.findall(r"\b[a-zA-Z]+\b", text.lower())

    # åˆå¹¶å¹¶å»é‡
    normalized = list(set(entities + numbers))
    return " ".join(sorted(normalized))  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´

# --------------------- åŠ¨æ€è¯„åˆ†æ¨¡å— ---------------------
def dynamic_comprehensive_evaluation(generated: str, reference: str) -> dict:
    """
    ç»¼åˆè¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼ˆç²¾ç¡®åŒ¹é… + æ¨¡ç³ŠåŒ¹é… + è¯­ä¹‰å¯†åº¦ï¼‰
    åŠ¨æ€è°ƒæ•´è¯„åˆ†æƒé‡
    """
    # è§„èŒƒåŒ–å¤„ç†
    norm_gen = normalize_answer(generated)
    norm_ref = normalize_answer(reference)

    # 1. ç²¾ç¡®åŒ¹é…
    exact_match = 1.0 if norm_ref == norm_gen else 0.0

     #2. æ¨¡ç³ŠåŒ¹é…ï¼ˆF1 Scoreï¼‰
    gen_tokens = set(norm_gen.split())
    ref_tokens = set(norm_ref.split())

    tp = len(gen_tokens & ref_tokens)
    precision = tp / len(gen_tokens) if gen_tokens else 0
    recall = tp / len(ref_tokens) if ref_tokens else 0
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)  # é˜²æ­¢é™¤é›¶

    # 3. è¯­ä¹‰å¯†åº¦
    semantic_density = compute_semantic_density(norm_gen, norm_ref)

    #åŠ¨æ€è°ƒæ•´æƒé‡
    if exact_match == 1.0:
        weight_exact = 1.0
        weight_f1 = 0
        weight_density = 0
    elif len(norm_ref.split()) <= 3:  # çŸ­ç­”æ¡ˆåé‡ç²¾ç¡®åŒ¹é…
        weight_exact = 0
        weight_f1 = 0.2
        weight_density = 0.8
    else:  # é•¿ç­”æ¡ˆåé‡è¯­ä¹‰åŒ¹é…
        weight_exact = 0
        weight_f1 = 0.05
        weight_density = 0.95

    '''if exact_match == 1.0:
        weight_exact = 1.0
        weight_density = 0
    else:
        weight_exact = 0
        weight_density=1.0'''

    # ç»¼åˆè¯„åˆ†
    final_score = (weight_exact * exact_match +
                   weight_f1 * f1_score +
                   weight_density * semantic_density)

    return {
        "exact_match": exact_match,
        "f1_score": f1_score,
        "semantic_density": semantic_density,
        "final_score": final_score,
        "is_correct": final_score >= 0.6  # é˜ˆå€¼å¯è°ƒæ•´
    }

# --------------------- åŸæœ‰é€»è¾‘å¢å¼º ---------------------
# åˆå§‹åŒ–NLIæ¨¡å‹
nli_model_name = "microsoft/deberta-large-mnli"
nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)
nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)

# Promptæ¨¡æ¿
PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšä¸”ä¸¥è°¨çš„åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªé—®é¢˜ï¼š
ã€é—®é¢˜ã€‘ï¼š{question}
è¯·å›ç­”è¿™ä¸ªé—®é¢˜ã€‚ç­”æ¡ˆè¶Šç®€å•è¶Šå¥½ï¼Œä¸éœ€è¦ä»»ä½•å¤šä½™è§£é‡Šã€‚ç”¨è‹±è¯­å›ç­”ï¼Œç­”æ¡ˆè¶Šç®€æ´è¶Šå¥½ï¼Œä¸éœ€è¦é‡å¤é—®å¥ä¸­çš„ä¿¡æ¯."""

# åŠ è½½æ•°æ®é›†
triviaqa_path = "triviaqa.jsonl"  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®è·¯å¾„
questions_to_evaluate = []
with open(triviaqa_path, "r", encoding="utf-8") as file:
    for idx, line in enumerate(file):
        if idx >= 20:  # åªåŠ è½½å‰20æ¡
            break
        entry = json.loads(line)
        questions_to_evaluate.append({
            "question": entry["question"],
            "reference_answer": entry["answer"],
            "is_impossible": entry.get("answer", "") == ""
        })

# OpenAIç”Ÿæˆå‡½æ•°
def generate_answer_with_api(question):
    client = ZhipuAI(api_key="4b58ba8916374a9ba1bf693f8224bf15.KhOOWDSO8ja8HK98")
    try:
        response = client.chat.completions.create(
            model="glm-4-plus",
            messages=[
                {"role": "system", "content": "ç›´æ¥å›ç­”é—®é¢˜ï¼Œç­”æ¡ˆåªéœ€åŒ…å«æ ¸å¿ƒä¿¡æ¯ã€‚"},
                {"role": "user", "content": PROMPT_TEMPLATE.format(question=question)}
            ],
            temperature=1.0,
            max_tokens=150
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"API Error: {str(e)}")
        return ""

# è¯­ä¹‰å¯†åº¦è®¡ç®—
def compute_semantic_density(generated, reference):
    inputs = nli_tokenizer(generated, reference, return_tensors="pt", truncation=True)
    outputs = nli_model(**inputs)
    probs = F.softmax(outputs.logits, dim=1)
    p_entailment = probs[:, 2].item()
    p_neutral = probs[:, 1].item()
    p_contradiction = probs[:, 0].item()
    semantic_density = p_entailment + 0.5 * p_neutral
    return semantic_density

# --------------------- è¯„ä¼°æµç¨‹å¢å¼º ---------------------
results = []
for item in questions_to_evaluate:
    gen_answer = generate_answer_with_api(item["question"])
    if not gen_answer:
        continue

    # æ‰§è¡Œç»¼åˆè¯„ä¼°
    eval_result = dynamic_comprehensive_evaluation(
        generated=gen_answer,
        reference=item["reference_answer"]
    )

    # è®°å½•ç»“æœ
    results.append({
        "question": item["question"],
        "generated": gen_answer,
        "reference": item["reference_answer"],
        **eval_result,
        "is_impossible": item["is_impossible"]
    })

# --------------------- ç»“æœåˆ†æå¢å¼º ---------------------
def print_colored(text, color_code):
    """ç»ˆç«¯å½©è‰²è¾“å‡º"""
    print(f"\033[{color_code}m{text}\033[0m")

# ç»Ÿè®¡ç»¼åˆè¯„åˆ†çš„å¹³å‡åˆ†
average_final_score = sum(res["final_score"] for res in results) / len(results) if results else 0

print(f"\nç»¼åˆè¯„åˆ†å¹³å‡åˆ†: {average_final_score:.4f}")

print("=" * 60)

# è¯¦ç»†ç»“æœå±•ç¤º
for res in results[:200]:  # æ‰“å°å‰20ä¸ªç¤ºä¾‹
    color_code = "32" if res["is_correct"] else "31"
    print_colored(f"é—®é¢˜ï¼š{res['question']}", color_code)
    print(f"ç”Ÿæˆç­”æ¡ˆï¼š{res['generated']}")
    print(f"å‚è€ƒç­”æ¡ˆï¼š{res['reference']}")
    print(f"ç²¾ç¡®åŒ¹é…ï¼š{res['exact_match']:.4f} | F1åˆ†æ•°ï¼š{res['f1_score']:.4f} | è¯­ä¹‰å¯†åº¦ï¼š{res['semantic_density']:.4f}")
    #print(f"ç²¾ç¡®åŒ¹é…ï¼š{res['exact_match']:.4f} | è¯­ä¹‰å¯†åº¦ï¼š{res['semantic_density']:.4f}")
    print(f"ç»¼åˆè¯„åˆ†ï¼š{res['final_score']:.4f} | åˆ¤å®šç»“æœï¼š{'æ­£ç¡®' if res['is_correct'] else 'é”™è¯¯'}")
    print("-" * 60)


def generate_separate_charts(results, save_dir="visualization_results"):
    """
    ç”Ÿæˆå¹¶ä¿å­˜å››å¼ ç‹¬ç«‹å›¾è¡¨åˆ°æŒ‡å®šç›®å½•
    è¿”å›ä¿å­˜æˆåŠŸçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # åˆ›å»ºå­˜å‚¨ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(save_dir, exist_ok=True)

    # å‡†å¤‡æ•°æ®
    final_scores = [r["final_score"] for r in results]
    exact_matches = [r["exact_match"] for r in results]
    f1_scores = [r["f1_score"] for r in results]
    semantic_densities = [r["semantic_density"] for r in results]
    correctness = [r["is_correct"] for r in results]

    saved_files = []

    try:
        # 1. ç»¼åˆè¯„åˆ†åˆ†å¸ƒç›´æ–¹å›¾
        plt.figure(figsize=(10, 6))
        plt.hist(final_scores, bins=20, color='skyblue', edgecolor='black', alpha=0.8)
        plt.title('Final Score Distribution', fontsize=14)
        plt.xlabel('Score', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.grid(True, alpha=0.3)
        hist_path = os.path.join(save_dir, "score_distribution.png")
        plt.savefig(hist_path, dpi=120, bbox_inches='tight')
        plt.close()
        saved_files.append(hist_path)

        # 2. æ­£ç¡®ç‡é¥¼å›¾
        plt.figure(figsize=(8, 8))
        correct_count = sum(correctness)
        sizes = [correct_count, len(results) - correct_count]
        explode = (0.05, 0)  # çªå‡ºæ˜¾ç¤ºæ­£ç¡®éƒ¨åˆ†
        plt.pie(sizes, labels=['Correct', 'Incorrect'], autopct='%1.1f%%',
                colors=['#66b3ff', '#ff9999'], startangle=90, explode=explode,
                textprops={'fontsize': 12}, shadow=True)
        plt.title('Accuracy Ratio', fontsize=14)
        pie_path = os.path.join(save_dir, "accuracy_pie.png")
        plt.savefig(pie_path, dpi=120, bbox_inches='tight')
        plt.close()
        saved_files.append(pie_path)

        # 3. æŒ‡æ ‡é›·è¾¾å›¾
        plt.figure(figsize=(10, 10))
        ax = plt.subplot(111, polar=True)
        categories = ['Exact Match', 'F1 Score', 'Semantic\nDensity']
        avg_values = [
            np.mean(exact_matches),
            np.mean(f1_scores),
            np.mean(semantic_densities)
        ]
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        avg_values += avg_values[:1]
        angles += angles[:1]

        ax.plot(angles, avg_values, color='#4CAF50', linewidth=3, linestyle='solid')
        ax.fill(angles, avg_values, color='#4CAF50', alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=12)
        ax.set_rlabel_position(30)
        plt.title('Metrics Radar Chart', fontsize=14, y=1.08)
        radar_path = os.path.join(save_dir, "metrics_radar.png")
        plt.savefig(radar_path, dpi=120, bbox_inches='tight')
        plt.close()
        saved_files.append(radar_path)

        # 4. è¯­ä¹‰å¯†åº¦æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(semantic_densities, final_scores,
                              c=final_scores, cmap='viridis',
                              alpha=0.7, edgecolors='w', linewidth=0.5)
        plt.colorbar(scatter, label='Final Score', shrink=0.8)
        plt.xlabel('Semantic Density', fontsize=12)
        plt.ylabel('Final Score', fontsize=12)
        plt.title('Semantic Density vs Final Score', fontsize=14)
        plt.grid(True, alpha=0.3)
        scatter_path = os.path.join(save_dir, "semantic_scatter.png")
        plt.savefig(scatter_path, dpi=120, bbox_inches='tight')
        plt.close()
        saved_files.append(scatter_path)

    except Exception as e:
        print_colored(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}", "31")
        return []

    return saved_files


# --------------------- åœ¨ç»“æœåˆ†æåè°ƒç”¨ ---------------------
# æ›¿æ¢åŸæœ‰çš„ generate_visualizations è°ƒç”¨
output_dir = "evaluation_charts"  # å¯è‡ªå®šä¹‰ç›®å½•åç§°
saved_files = generate_separate_charts(results, save_dir=output_dir)

if saved_files:
    print_colored(f"\nğŸ“Š å››å¼ å›¾è¡¨å·²ä¿å­˜è‡³ {output_dir} ç›®å½•ï¼š", "32")
    for path in saved_files:
        print_colored(f"âœ… {os.path.basename(path)}", "36")
else:
    print_colored("\nâš ï¸ å›¾è¡¨ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯", "31")